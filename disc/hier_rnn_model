import tensorflow as tf
import numpy as np

class Hier_rnn_model(object):
    def __init__(self, config, scope_name="hier_rnn_model"):
        with tf.variable_scope(name_or_scope=scope_name):
            emb_dim = config.embed_dim
            hidden_layer_num = config.hidden_layer_num
            vocab_size = config.vocabulary_size
            max_len = config.max_len
            class_num = config.class_num
            batch_size = config.batch_size
            self.lr = config.lr
            self.global_step = tf.Variable(initial_value=0, trainable=False)

            self.query = []
            self.answer = []
            for i in range(max_len):
                self.query.append(tf.placeholder(dtype=tf.int32, shape=[None], name="query{0}".format(i)))
                self.answer.append(tf.placeholder(dtype=tf.int32, shape=[None], name="answer{0}".format(i)))

            self.target = tf.placeholder(dtype=tf.int64, shape=[None], name="target")
            #self.forward_only = tf.placeholder(dtype=tf.bool, name="forward_only")

            #self.query = tf.placeholder(dtype=tf.int32, shape=None, name="query")
            #self.answer = tf.placeholder(dtype=tf.int32, shape=None, name="answer")

            encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(emb_dim)
            encoder_mutil = tf.nn.rnn_cell.MultiRNNCell([encoder_cell] * hidden_layer_num)
            encoder_emb = tf.nn.rnn_cell.EmbeddingWrapper(encoder_mutil, embedding_classes=vocab_size, embedding_size=emb_dim)

            #self._initial_state = cell.zero_state(self.batch_size, dtype=tf.float32)

            #self._init_state = encoder_emb.zero_state(batch_size=batch_size, dtype=tf.float32)

            with tf.variable_scope(name_or_scope="Hier_RNN") as var_scope:
                query_output, self.query_state = tf.nn.rnn(encoder_emb, inputs=self.query, dtype=tf.float32)
                # output [max_len, batch_size, emb_dim]   state [num_layer, 2, batch_size, emb_dim]
                var_scope.reuse_variables()
                answer_output, self.answer_state = tf.nn.rnn(encoder_emb, inputs=self.answer, dtype=tf.float32)

            self.context_input = [self.query_state[-1][1], self.answer_state[-1][1]]

            context_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=emb_dim)
            context_multi = tf.nn.rnn_cell.MultiRNNCell([context_cell] * hidden_layer_num)
            #context_emb = tf.nn.rnn_cell.EmbeddingWrapper(context_multi, embedding_classes=vocab_size, embedding_size=emb_dim)
            output, self.state = tf.nn.rnn(context_multi, self.context_input, dtype=tf.float32)

            self.top_state = self.state[-1][1]  # [batch_size, emb_dim]

            with tf.name_scope("Softmax_layer_and_output"):
                softmax_w = tf.get_variable("softmax_w", [emb_dim, class_num], dtype=tf.float32)
                softmax_b = tf.get_variable("softmax_b", [class_num], dtype=tf.float32)
                self.logits = tf.matmul(self.top_state, softmax_w) + softmax_b

            with tf.name_scope("loss"):
                self.loss = tf.nn.sparse_softmax_cross_entropy_with_logits(self.logits, self.target)
                self.cost = tf.reduce_mean(self.loss)

            with tf.name_scope("gradient_descent"):
                disc_params = [var for var in tf.trainable_variables() if scope_name in var.name]
                grads, norm = tf.clip_by_global_norm(tf.gradients(self.cost, disc_params), config.max_grad_norm)
                optimizer = tf.train.GradientDescentOptimizer(self.lr)
                self.train_op = optimizer.apply_gradients(zip(grads, disc_params), global_step=self.global_step)

            all_variables = [v for v in tf.global_variables() if scope_name in v.name]
            self.saver = tf.train.Saver(all_variables)

class Config(object):
    emb_dim = 12
    class_num = 2
    hidden_layer_num = 3
    vocabulary_size = 10
    max_len = 5
    batch_size = 1
    init_scale = 0.1


def main(_):
    with tf.Session() as sess:
        query = [[1],[2],[3],[4],[5]]
        answer = [[6],[7],[8],[9],[0]]
        target = [1]
        config = Config
        initializer = tf.random_uniform_initializer(-1 * config.init_scale, 1 * config.init_scale)
        with tf.variable_scope(name_or_scope="rnn_model", initializer=initializer):
            model = Hier_rnn_model(config)
            sess.run(tf.global_variables_initializer())
        input_feed = {}
        for i in range(config.max_len):
            input_feed[model.query[i].name] = query[i]
            input_feed[model.answer[i].name] = answer[i]
        input_feed[model.target.name] = target

        fetches = [model.query_state, model.context_input, model.query_state, model.top_state, model.logits]

        query, context, state, top, logits = sess.run(fetches=fetches, feed_dict=input_feed)

        print("query: ", np.shape(query))
        print("context: ", np.shape(context))

    pass

if __name__ == '__main__':
    tf.app.run()
